defaults:
  - trainer: pl_trainer
  - engine: predictor
  - model: tts_imp
  - embedding: periodic
  - regularization: none
  - _self_

hydra:
  job:
    chdir: true
  run:
    dir: "outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}"
  sweep:
    dir: "outputs/multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}"
    subdir: "${hydra.job.num}"

trainer:
  max_epochs: 150
  limit_train_batches: 500

early_stopping:
  patience: 50
  grace_epochs: 0

dataset:
  target: pems8
  batch_size: 64
  num_workers: 0
  splitting:
    val_len: 0.2
    test_len: 0.2
  window: 12
  horizon: 12
  stride: 1
  connectivity:
    method: binary
    layout: csr
    include_self: false
  add_exogenous: false

model:
  cached: false
  add_backward: true

embedding:
  module:
    emb_size: 32
    initializer: "uniform"
  forgetting_scheduler:
    warmup_for: 5
    stop_after: 96

engine:
  optim_kwargs:
    lr: 0.005
  scheduler_class: null
  scheduler_kwargs: null
  scale_target: false
  metrics:
    mae@25:
      _target_: "tsl.metrics.torch.MaskedMAE"
      at: "${math: '${dataset.horizon} // 4 - 1'}"
    mae@50:
      _target_: "tsl.metrics.torch.MaskedMAE"
      at: "${math: '${dataset.horizon} // 2 - 1'}"
    mae@100:
      _target_: "tsl.metrics.torch.MaskedMAE"
      at: "${math: '${dataset.horizon} - 1'}"

target:
  splitting:
    test_len: 0.2
    val_len: 2016 # one week: 5 min * 12 * 24 * 7
    train_len:
      - 4032  # two weeks: 5 min * 12 * 24 * 7 * 2
      - 2016  # one week: 5 min * 12 * 24 * 7
      - 864   # three days: 5 min * 12 * 24 * 3
      - 288   # one day: 5 min * 12 * 24
      - 0     # zero shot
  log_test_len:
    - 2016  # one week: 5 min * 12 * 24 * 7
  engine:
    optim_kwargs:
      lr: 0.001
    scheduler_class: null
    scheduler_kwargs: null
  trainer:
    max_epochs: 1000
    limit_train_batches: 1.0
  early_stopping:
    patience: 100
  tune_all: false
  reset_from_learned: false

seed: 1
checkpoint: null
monitor: "val/loss"
num_threads: 1
log_run: "embreg_transfer"